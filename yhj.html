
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Han-Jia Ye @ LAMDA, NJU-CS</title>
<style type="text/css">
BODY {
	BACKGROUND-COLOR: #ffffff
}
.style1 {
	FONT-SIZE: 16px; FONT-FAMILY: "Times New Roman", Times, serif
}
.style2 {
	FONT-WEIGHT: bold; FONT-SIZE: 16px
}
.style7 {
	FONT-WEIGHT: bold; FONT-SIZE: x-large; FONT-FAMILY: Arial, Helvetica, sans-serif
}
.style8 {
	font-family: "楷体";
	font-size: 36px;
}
.style9 {
	FONT-SIZE: 18px
}
.style12 {
	FONT-SIZE: 20px
}
.style13 {
	FONT-SIZE: 12px; FONT-FAMILY: Arial, Helvetica, sans-serif
}
.style17 {
	FONT-WEIGHT: bold; FONT-SIZE: 16px; FONT-FAMILY: Geneva, Arial, Helvetica, sans-serif
}
.STYLE18 {
	font-family: “黑体”
	font-size: 36px;
}
</style>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Han-Jia Ye @ LAMDA, NJU</h1>
</div>
<table class="imgtable"><tr><td>
<a href="IMGLINKTARGET"><img src="imgs/yehj3.jpg" alt="Han-Jia Ye" width="321.0px" height="214.0" /></a>&nbsp;</td>
<td align="left"><span class="style8">叶翰嘉</span><br />
<span class="style7">Han-Jia Ye (H.-J. YE)</span><br />
Assistant Researcher<br />
<a href="http://lamda.nju.edu.cn/">LAMDA Group</a><br />
<a href="http://ai.nju.edu.cn/">School of Artificial Intelligence</a><br />
<a href="http://www.nju.edu.cn/">Nanjing University</a>, Nanjing 210023, China.<br /></p>
<p><b>Email</b>: yehj [at] lamda.nju.edu.cn<br /></b>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspyehj [at] nju.edu.cn<br/>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp yhjyehanjia [at] gmail.com</p>
</td>
<td valign="top" width="179"><a href="http://lamda.nju.edu.cn/" target="_blank"><img height="85" src="imgs/lamda.jpg" width="179" border="0"></a></td>
<td valign="top" width="100"><a href="http://www.nju.edu.cn/" target="_blank"><img height="120" src="imgs/nju_badge.jpg" width="100" border="0"></a></td></tr></table>
<h2>Short Bio</h2>
<ul>
<li>Han-Jia Ye is an Assistant Researcher in the <a href="http://ai.nju.edu.cn/">School of Artificial Intelligence</a> at the <a href="http://www.nju.edu.cn/">Nanjing University</a> (NJU). His major research focuses on machine learning and its applications to data mining and computer vision. </li>

<li>Han-Jia received his B.Sc. degree from <a href="http://www.njupt.edu.cn/">Nanjing University of Posts and Telecommunications</a>, China in June 2013. After that, he became an M.Sc. student in the LAMDA Group led by professor <a href="http://cs.nju.edu.cn/zhouzh/">Zhi-Hua Zhou</a> in Nanjing University. From Sept. 2015, Han-Jia started his Ph.D. degree in machine learning under the supervision of Prof. <a href="http://www.lamda.nju.edu.cn/jiangy/">Yuan Jiang</a> and Prof. <a href="http://www.lamda.nju.edu.cn/zhandc/">De-Chuan Zhan</a>. He received his PhD degree at May 2019.</li>
</ul>

<h2>Latest News</h2>
<ul>
<li><p>09/2020: 1 paper accepted by IJCV on generalized few-shot learning.</p>

<li><p>04/2020: 1 paper accepted by TPAMI on heterogeneous few-shot model reuse.</p>

<li><p>03/2020: 2 papers (1 oral and 1 poster) accepted by CVPR 2020.</p>

<li><p>02/2020: 1 <a href="https://arxiv.org/abs/2002.00573">arXiv paper</a> on meta-learning.</p>

<li><p>01/2020: 1 <a href="https://arxiv.org/abs/2001.01385">arXiv paper</a> on imbalanced deep learning.</p>

<li><p>11/2019: Attending <a href="http://www.acml-conf.org/2019/">ACML 2019</a> in Nagoya, Japan.</p>

<li><p>10/2019: 1 paper accepted by TKDE on multiple instance learning w/ novel class.</p>

<li><p>09/2019: Invited talk at a <a href="http://grid.hust.edu.cn/bigdata2019/content/index.html">CCF-Big Data</a> workshop (Wuhan, China) on "Multi-Metric Learning for Heterogeneous Data".</p>

<li><p>09/2019: One manuscript with Xiang-Rong Sheng and De-Chuan Zhan is accepted by Machine Learning.</p>

<li><p>07/2019: Joining the Nanjing University (School of Artificial Intelligence) as an Assistant Researcher.</p>

<li><p>05/2019: Successfully defending thesis on "Metric Learning for Open Environment".</p>

<li><p>10/2018: Finished the visiting at Prof. <a href="http://www-bcf.usc.edu/~feisha/">Fei Sha</a>'s group in University of Southern California, LA.</p>

</li>
</ul>

<h2>Main Research Interests</h2>
<p style="color:#0000FF;font-family:Palatino Linotype"><b>Learning with Similarity and Distance</b></p>
<ul>
<li><p>Han-Jia focuses on finding an <b>adaptive similarity/distance measure</b> between objects to reflect their relationships, i.e., comparing examples in a better way.<br />
Similarity and distance measurement constructs the basis of many learning methods and facilitates real applications as well. Han-Jia analyzes the theoretical foundations of learning a distance measure and explores a unified view to explain complex linkages between objects.
</p>
</li>
</ul>
<p style="color:#0000FF;font-family:Palatino Linotype"><b>Learning with Limited Data</b></p>
<ul>
<li><p>The ability of a model to <b>fit with limited data</b> is essential and necessary due to the instance/label collection cost. How to extract and utilize knowledge from related tasks and domains is the key. Specifically, Han-Jia mainly works on two directions: how to reuse model effectively across heterogeneous tasks, and how to learn meta-knowledge for few-shot learning.

</p>
</li>
</ul>
<p style="color:#0000FF;font-family:Palatino Linotype"><b>Learning with Rich Semantics</b></p>
<ul>
<li><p>Real-world <b>complex environments</b> usually involve complex semantics. Han-Jia trys to discover semantic information from data following a three-step strategy, i.e., combining multiple data sources, exploring/decomposing types of relationship between objects, and automatically selecting over suitable semantic component. 
</p>
</li>
</ul>

<h2>Publications (Preprints)</h2>
<table class="imgtable"><tr><td>
<img src="imgs/CDT.png" alt="WSFG" width="300px" height="120px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, Hong-You Chen, De-Chuan Zhan, Wei-Lun Chao. Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning. arXiv:2001.01385, 2020. <a href=https://arxiv.org/abs/2001.01385>[Paper]</p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">We identify the over-fitting and feature divation phenomena for minor classes in deep long-tail learning, and propose a simple yet effective class-dependent temperature to compensate for such influence.</p>
</ul>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="imgs/CASTLE.png" alt="WSFG" width="300px" height="90px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>*, Hexiang Hu*, De-Chuan Zhan, Fei Sha. Learning Classifier Synthesis for Generalized Few-Shot Learning. arXiv:1906.02944, 2019. <a href=https://arxiv.org/abs/1906.02944>[Paper]</p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">We investigate the problem of generalized few-shot learning and propose a learning framework that learns how to synthesize calibrated few-shot classifiers in addition to the multi-class classifiers of "head" classes.</p>
</ul>
</td></tr></table>

<h2>Publications (Conference Papers)</h2>
<table class="imgtable"><tr><td>
<img src="imgs/ReFilled.png" alt="WSFG" width="300px" height="120px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, Su Lu, De-Chuan Zhan. Distilling Cross-Task Knowledge via Relationship Matching. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'20), Seattle, Washington, 2020. To appear. <a href="papers/ReFilled.pdf">[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">To reuse the cross-task knowledge, we distill the comparison ability and the local classification ability of the embedding and the top-layer classifier from a teacher model, respectively.</p>
</ul>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="imgs/FEAT.png" alt="WSFG" width="300px" height="120px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, Hexiang Hu, De-Chuan Zhan, Fei Sha. Learning Embedding Adaptation for Few-Shot Learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'20), Seattle, Washington, 2020. To appear. <a href=https://arxiv.org/abs/1812.03664>[Paper]</a><a href=https://github.com/Sha-Lab/FEAT>[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">For few-shot learning, we employ a type of self-attention mechanism to transform the embeddings from task-agnostic to task-specific in both seen and unseen classes.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/Meta-Learning.png" alt="WSFG" width="300px" height="120px" />&nbsp;</td>
<td align="left"><ul>
<li><p>Wei-Lun Chao*, <b>Han-Jia Ye</b>*, De-Chuan Zhan, Mark Campbell, Kilian Q. Weinberger. A Meta Understanding of Meta-Learning. In: The Adaptive and Multitask Learning (AMTL) 2019 Workshop, Long Beach, CA, 2019. <a href=https://openreview.net/pdf?id=Syg5V5ri2V>[Paper]</a> <a href=https://arxiv.org/abs/2002.00573>[ArXiv]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">By rethinking meta-learning as a kind of supervised learning, we can borrow supervised learning tricks for the meta-learning paradigm.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/ReForm.png" alt="WSFG" width="300px" height="120px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Yuan Jiang, Zhi-Hua Zhou. Rectify Heterogeneous Models with Semantic Mapping. In: Proceedings of the 35th International Conference on Machine Learning (ICML'18), Stockholm, Sweden, 2018. Page: 5630-5639. <a href="papers/reform.pdf">[Paper]</a><a href=http://www.lamda.nju.edu.cn/code_ReForm.ashx>[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">The reusability and evovability of a model are anlayzed in this paper. The proposed framework generates meta features and reuses model across heterogeneous feature domains.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/MapHere.png" alt="WSFG" width="300px" height="140px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, Xiang-Rong Sheng, De-Chuan Zhan, Peng He. Distance Metric Facilitated Transportation between Heterogeneous Domains. In: Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI'18), Stockholm, Sweden, 2018. Page: 3012-3018. <a href="papers/MapHere.pdf">[Paper]</a><a href="code/Maphere.zip">[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">We deal with a specific problem for Optimal Transport Domain Adaptation. Our method extends the ability of OTDA to heterogeneous domains.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/DRIFT_Illustration.jpg" alt="WSFG" width="300px" height="140px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Xue-Min Si, Yuan Jiang. Learning Mahalanobis Distance Metric: Considering Instance Disturbance Helps. In: Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI'17), Melbourne, Australia, 2017. Page: 3315-3321. <a href="papers/DRIFT.pdf">[Paper]<a href="code/DRIFT_Code.zip">[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A robust Mahalanobis distance metric learning approach dealing with both instance and side-information uncertainty effectively.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/CS3G_Illustration.jpg" alt="WSFG" width="300px" height="140px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Xiaolin Li, Zhen-Chuan Huang, Yuan Jiang. College Student Scholarships and Subsidies Granting: A Multi-Modal Multi-Label Approach. In: Proceedings of the IEEE International Conference on Data Mining (IEEE ICDM'16), Barcelona, Spain, 2016, Page: 559–568. <a href="papers/ICDM2016-CS3G.pdf">[Paper]</a><a href="code/CS3G.zip">[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A multi-modal and multi-label method dealing with real-world college student scholarships and subsidies granting task.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/UM2L_Illustration.jpg" alt="WSFG" width="300px" height="130px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Xue-Min Si, Yuan Jiang, Zhi-Hua Zhou.  What Makes Objects Similar: A Unified Multi-Metric Learning Approach. In: Advances in Neural Information Processing Systems 29 (NIPS'16), Barcelona, Spain, 2016, Page: 1235-1243. <a href="papers/NIPS16-UM2L.pdf">[Paper]</a><a href="code/UM2L Code.zip">[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A unified multi-metric learning approach discovering various types of semantics under objects linkages. Besides, we provide a unified solver with theoretical guarantee.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/FARM_Illustration.png" alt="WSFG" width="300px" height="150px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, Xue-Min Si, De-Chuan Zhan, Yuan Jiang.  Learning Feature Aware Metric. In: Proceedings of the 8th Asian Conference on Machine Learning (ACML'16), Hamilton, New Zealand, 2016, Page: 286–301. <a href="papers/ye4.pdf">[Paper]</a><a href="code/FARM.zip">[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A fast decomposition strategy learning sparse/robust Mahalanobis distance metric.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/ISMETS_Illustration.png" alt="WSFG" width="300px" height="150px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Yuan Jiang. Instance Specific Metric Subspace Learning: A Bayesian Approach. In: Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16), Phoenix, AZ, 2016, Page: 2272-2278. <a href="papers/ye-zhan.pdf">[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A Bayesian perspective of distance metric learning, which can infer metric inductively.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/RANC_Illustration.png" alt="WSFG" width="300px" height="150px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Yuan Miao, Yuan Jiang, Zhi-Hua Zhou. Rank Consistency based Multi-View Learning: A Privacy-Preserving Approach. In: Proceedings of the 24th ACM International Conference on Information and Knowledge Management (<a href="http://www.cikm-2015.org/">CIKM'15</a>), Melbourne, Australia, 2015, Page: 991-1000. <a href="papers/CIKM15-RanC.pdf">[Paper]<a href="code/RANC code.zip">[code]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A novel rank consistency criterion is proposed for multi-view learning in a privacy-preserving scenario.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/ARM_Illustration.png" alt="WSFG" width="300px" height="150px" />&nbsp;</td>
<td align="left"><ul>
<li><p>Yang Yang, <b>Han-Jia Ye</b>, De-Chuan Zhan, Yuan Jiang. Auxiliary Information Regularized Machine for Multiple Modality Feature Learning. In: Proceedings of the 24th International Joint Conference on Artificial Intelligence (<a href="http://ijcai-15.org/">IJCAI'15</a>), Buenos Aires, Argentina, 2015, Page: 1033-1039. <a href="papers/ARM.pdf">[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">Improve the prediction ability of cheap weak modal feature with the help of its strong counterpart.</p>
</ul>
</td></tr></table>

<h2>Publications (Journal Papers)</h2>
<table class="imgtable"><tr><td>
<img src="imgs/Aviator.png" alt="WSFG" width="300px" height="100px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, Xiang-Rong Sheng, De-Chuan Zhan. Few-shot learning with adaptively initialized task optimizer: a practical meta-learning approach. Machine Learning. 2020, Volume 109, pp 643–664. <a href=https://link.springer.com/article/10.1007/s10994-019-05838-7>[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A practical meta-learning approach which efficiently adapts the task-specific initialization to an effective classifier.</p>
</ul>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="imgs/MIEN.png" alt="WSFG" width="300px" height="100px" />&nbsp;</td>
<td align="left"><ul>
<li><p>Xiu-Shen Wei*, <b>Han-Jia Ye</b>*, Xin Mu, Jianxin Wu, Chunhua Shen, Zhi-Hua Zhou. Multi-instance learning with emerging novel class. IEEE Transactions on Knowledge and Data Engineering. To appear. <a href=https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkde20mien.pdf>[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">A local metric learning approach to deal with the emerging novel class in multi-instance learning tasks.</p>
</ul>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="imgs/LIFT.png" alt="WSFG" width="300px" height="100px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Nan Li, Yuan Jiang. Learning Multiple Local Metrics: Global Consideration Helps. IEEE Transactions on Pattern Analysis and Machine Intelligence. To appear. <a href=https://ieeexplore.ieee.org/document/8653339>[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">By learning local metrics based on the global one, we try to adaptively allocate local metrics for heterogeneous data.</p>
</ul>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="imgs/MCP.png" alt="WSFG" width="300px" height="120px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan. Few-Shot Learning via Model Composition (in Chinese). In: SCIENTIA SINICA Informatics (中国科学：信息科学). April 2020, Volume 50, Issue 5. <a href=http://engine.scichina.com/doi/10.1360/N112018-00332>[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">We propose to compose classifiers inspired by the closed form of the least square loss, which fits learning with limited training examples.</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/MLJ-Theory.png" alt="WSFG" width="300px" height="140px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Yuan Jiang. Fast Generalization Rates for Distance Metric Learning. Machine Learning. February 2019, Volume 108, Issue 2, pp 267–295. <a href=https://link.springer.com/article/10.1007/s10994-018-5734-0>[Paper]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">Theoretical analysis of distance metric learning with fast generalization rate</p>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/SemanticMetric.png" alt="WSFG" width="300px" height="140px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Han-Jia Ye</b>, De-Chuan Zhan, Yuan Jiang, Zhi-Hua Zhou. What Makes Objects Similar: A Unified Multi-Metric Learning Approach. IEEE Transactions on Pattern Analysis and Machine Intelligence. May 2019, Volume 41, Issue 5, pp 1257-1270. <a href=https://ieeexplore.ieee.org/document/8344546>[Paper]</a><a href="papers/UM2L_supp.pdf">[Supplementary]</a></p>
</li>
<p style="color:#B03A2E;font-family:Palatino Linotype">This manuscript extends our NIPS work. The concept of semantic metric, generalization analysis, and deep extension are introduced to get a more general framework.</p>
</ul>
</td></tr></table>
<li><b>叶翰嘉</b>, 詹德川. 度量学习研究进展. 中国人工智能学会通讯，2017,12:02-07. <a href="metric_learning.pdf">[Paper]</a>
</li>
<li>Xiaochuan Zou, <b>Han-Jia Ye</b>, De-Chuan Zhan. Image Classification and Concept Detection based on Strong and Weak Modality (in chinese with english abstract). Journal of Nanjing University, 2014,02:228-234.
</li>

<h2>Journal and Conference Reviewer</h2>
TPAMI, TKDE, TKDD, TNNLS, Neurocomputing, AAAI 2021, IJCAI 2021, ECML/PKDD 2021, NeurIPS 2020, ICDM 2020, CVPR 2020, IJCAI 2020, NeurIPS 2019, CVPR 2019, ICCV 2019, IJCAI 2019, AAAI 2019, ACML 2019, ICLR 2019, NeurIPS 2018, ACML 2018, AAAI 2018, IJCAI 2018, CIKM 2017, IJCAI 2017, KDD 2017, PAKDD 2017, SDM 2017, AISTATS 2017, AAAI 2017, NIPS 2016, IJCAI 2016, ICPR 2016, AAAI 2015, IJCAI 2015, PAKDD 2015

<h2>Course</h2>
<ul>
<li><p><a href="https://www.lamda.nju.edu.cn/yehj/dsp2020">Digital Singal Processing</a>. (For undergraduate and graduate students, Autumn, 2020)</p>
</li>
<li><p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/course/ml.htm">Introduction to Machine Learning</a>. (With Prof. Zhi-Hua Zhou and Prof. De-Chuan Zhan; For undergraduate and graduate students, Spring, 2020)</p>
</li>
</ul>
<h2>Correspondence</h2>
<p>
Office:	Room A205, Yifu Building, Xianlin Campus of Nanjing University<br />
Address: Han-Jia Ye<br />
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspNational Key Laboratory for Novel Software Technology <br />
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspNanjing University, Xianlin Campus Mailbox 603 <br />
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp163 Xianlin Avenue, Qixia District, Nanjing 210046, China</p>
<div id="footer">
	<div class="foot">
		<span> Last modified by <a href="index.html#top">Han-Jia Ye</a> on Sept 6, 2020. </span>
	</div>
</div>
</body>
</html>
